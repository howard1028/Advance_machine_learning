{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8Vrdrxo+9eZcSh+XVjr3N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/howard1028/Advance_machine_learning/blob/main/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfETI1lu38YF"
      },
      "outputs": [],
      "source": [
        "#hw1-2\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print('GPU state:', device)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGX_mkB9vGLi",
        "outputId": "fd64109c-4d39-441f-91d2-c7bef53a235d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU state: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "# GPU\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print('GPU state:', device)\n",
        "\n",
        "\n",
        "# Cifar-10 data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "# Data\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "trainLoader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=0)\n",
        "testLoader = torch.utils.data.DataLoader(testset, batch_size=8, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "# Data classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqKp6NdsvLMw",
        "outputId": "7db768c5-ddd4-43f5-ee09-6266599d2ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU state: cuda:0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model structure\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = (7,7), stride = (1, 1), padding = (3, 3))\n",
        "        self.conv2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (5,5), stride = (1, 1), padding = (2, 2))\n",
        "        \n",
        "        self.max_pooling = nn.MaxPool2d(kernel_size = (2, 2),stride = (2, 2))\n",
        "        # self.dropout = nn.Dropout(p = 0.1)\n",
        "        \n",
        "        # self.padding1 = nn.ZeroPad2d(8)\n",
        "        # self.padding2 = nn.ZeroPad2d((9, 8, 1, 1))\n",
        "\n",
        "        self.fc1 = nn.Linear(128*8*8, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        #self.Softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.max_pooling(x)\n",
        "        # x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        # x = self.padding2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.max_pooling(x)\n",
        "        # x = self.padding1(x)\n",
        "        # x = self.dropout(x)\n",
        "\n",
        "        x = x.view(-1, 128*8*8) # -1 -> 不指定flatten的大小\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        #x = self.Softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "net = Model().to(device)\n",
        "print(net) # 印出神經網路的相關資訊"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ3RXXV5x8qC",
        "outputId": "27839d19-4b26-4c2d-f1b0-75626d566fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (max_pooling): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=8192, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parameters\n",
        "criterion = nn.CrossEntropyLoss() # 使用 cross entropy作為loss function之方式\n",
        "lr = 0.001 # learning rate\n",
        "epochs = 10 # 訓練3次\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "\n",
        "\n",
        "# Train\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for times, data in enumerate(trainLoader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if times % 100 == 99 or times+1 == len(trainLoader):\n",
        "            print('[%d/%d, %d/%d] loss: %.3f' % (epoch+1, epochs, times+1, len(trainLoader), running_loss/100))\n",
        "            running_loss = 0\n",
        "\n",
        "print('Finished Training\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USJ2GIaeyLou",
        "outputId": "55225095-6b88-43ba-c42c-fe99c3b14129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/10, 100/6250] loss: 0.201\n",
            "[1/10, 200/6250] loss: 0.137\n",
            "[1/10, 300/6250] loss: 0.196\n",
            "[1/10, 400/6250] loss: 0.159\n",
            "[1/10, 500/6250] loss: 0.204\n",
            "[1/10, 600/6250] loss: 0.153\n",
            "[1/10, 700/6250] loss: 0.180\n",
            "[1/10, 800/6250] loss: 0.150\n",
            "[1/10, 900/6250] loss: 0.181\n",
            "[1/10, 1000/6250] loss: 0.175\n",
            "[1/10, 1100/6250] loss: 0.143\n",
            "[1/10, 1200/6250] loss: 0.153\n",
            "[1/10, 1300/6250] loss: 0.130\n",
            "[1/10, 1400/6250] loss: 0.179\n",
            "[1/10, 1500/6250] loss: 0.151\n",
            "[1/10, 1600/6250] loss: 0.162\n",
            "[1/10, 1700/6250] loss: 0.139\n",
            "[1/10, 1800/6250] loss: 0.148\n",
            "[1/10, 1900/6250] loss: 0.174\n",
            "[1/10, 2000/6250] loss: 0.154\n",
            "[1/10, 2100/6250] loss: 0.174\n",
            "[1/10, 2200/6250] loss: 0.169\n",
            "[1/10, 2300/6250] loss: 0.216\n",
            "[1/10, 2400/6250] loss: 0.201\n",
            "[1/10, 2500/6250] loss: 0.218\n",
            "[1/10, 2600/6250] loss: 0.156\n",
            "[1/10, 2700/6250] loss: 0.178\n",
            "[1/10, 2800/6250] loss: 0.171\n",
            "[1/10, 2900/6250] loss: 0.195\n",
            "[1/10, 3000/6250] loss: 0.197\n",
            "[1/10, 3100/6250] loss: 0.193\n",
            "[1/10, 3200/6250] loss: 0.195\n",
            "[1/10, 3300/6250] loss: 0.200\n",
            "[1/10, 3400/6250] loss: 0.203\n",
            "[1/10, 3500/6250] loss: 0.214\n",
            "[1/10, 3600/6250] loss: 0.188\n",
            "[1/10, 3700/6250] loss: 0.210\n",
            "[1/10, 3800/6250] loss: 0.204\n",
            "[1/10, 3900/6250] loss: 0.173\n",
            "[1/10, 4000/6250] loss: 0.158\n",
            "[1/10, 4100/6250] loss: 0.197\n",
            "[1/10, 4200/6250] loss: 0.226\n",
            "[1/10, 4300/6250] loss: 0.256\n",
            "[1/10, 4400/6250] loss: 0.266\n",
            "[1/10, 4500/6250] loss: 0.238\n",
            "[1/10, 4600/6250] loss: 0.254\n",
            "[1/10, 4700/6250] loss: 0.242\n",
            "[1/10, 4800/6250] loss: 0.196\n",
            "[1/10, 4900/6250] loss: 0.200\n",
            "[1/10, 5000/6250] loss: 0.206\n",
            "[1/10, 5100/6250] loss: 0.289\n",
            "[1/10, 5200/6250] loss: 0.245\n",
            "[1/10, 5300/6250] loss: 0.243\n",
            "[1/10, 5400/6250] loss: 0.213\n",
            "[1/10, 5500/6250] loss: 0.224\n",
            "[1/10, 5600/6250] loss: 0.261\n",
            "[1/10, 5700/6250] loss: 0.249\n",
            "[1/10, 5800/6250] loss: 0.244\n",
            "[1/10, 5900/6250] loss: 0.249\n",
            "[1/10, 6000/6250] loss: 0.230\n",
            "[1/10, 6100/6250] loss: 0.232\n",
            "[1/10, 6200/6250] loss: 0.325\n",
            "[1/10, 6250/6250] loss: 0.131\n",
            "[2/10, 100/6250] loss: 0.142\n",
            "[2/10, 200/6250] loss: 0.099\n",
            "[2/10, 300/6250] loss: 0.116\n",
            "[2/10, 400/6250] loss: 0.087\n",
            "[2/10, 500/6250] loss: 0.105\n",
            "[2/10, 600/6250] loss: 0.135\n",
            "[2/10, 700/6250] loss: 0.146\n",
            "[2/10, 800/6250] loss: 0.114\n",
            "[2/10, 900/6250] loss: 0.111\n",
            "[2/10, 1000/6250] loss: 0.110\n",
            "[2/10, 1100/6250] loss: 0.155\n",
            "[2/10, 1200/6250] loss: 0.141\n",
            "[2/10, 1300/6250] loss: 0.125\n",
            "[2/10, 1400/6250] loss: 0.126\n",
            "[2/10, 1500/6250] loss: 0.116\n",
            "[2/10, 1600/6250] loss: 0.108\n",
            "[2/10, 1700/6250] loss: 0.103\n",
            "[2/10, 1800/6250] loss: 0.155\n",
            "[2/10, 1900/6250] loss: 0.160\n",
            "[2/10, 2000/6250] loss: 0.159\n",
            "[2/10, 2100/6250] loss: 0.176\n",
            "[2/10, 2200/6250] loss: 0.128\n",
            "[2/10, 2300/6250] loss: 0.159\n",
            "[2/10, 2400/6250] loss: 0.155\n",
            "[2/10, 2500/6250] loss: 0.157\n",
            "[2/10, 2600/6250] loss: 0.121\n",
            "[2/10, 2700/6250] loss: 0.149\n",
            "[2/10, 2800/6250] loss: 0.172\n",
            "[2/10, 2900/6250] loss: 0.127\n",
            "[2/10, 3000/6250] loss: 0.156\n",
            "[2/10, 3100/6250] loss: 0.129\n",
            "[2/10, 3200/6250] loss: 0.151\n",
            "[2/10, 3300/6250] loss: 0.141\n",
            "[2/10, 3400/6250] loss: 0.167\n",
            "[2/10, 3500/6250] loss: 0.173\n",
            "[2/10, 3600/6250] loss: 0.143\n",
            "[2/10, 3700/6250] loss: 0.159\n",
            "[2/10, 3800/6250] loss: 0.210\n",
            "[2/10, 3900/6250] loss: 0.188\n",
            "[2/10, 4000/6250] loss: 0.172\n",
            "[2/10, 4100/6250] loss: 0.163\n",
            "[2/10, 4200/6250] loss: 0.181\n",
            "[2/10, 4300/6250] loss: 0.188\n",
            "[2/10, 4400/6250] loss: 0.154\n",
            "[2/10, 4500/6250] loss: 0.195\n",
            "[2/10, 4600/6250] loss: 0.175\n",
            "[2/10, 4700/6250] loss: 0.149\n",
            "[2/10, 4800/6250] loss: 0.212\n",
            "[2/10, 4900/6250] loss: 0.205\n",
            "[2/10, 5000/6250] loss: 0.191\n",
            "[2/10, 5100/6250] loss: 0.217\n",
            "[2/10, 5200/6250] loss: 0.181\n",
            "[2/10, 5300/6250] loss: 0.148\n",
            "[2/10, 5400/6250] loss: 0.221\n",
            "[2/10, 5500/6250] loss: 0.218\n",
            "[2/10, 5600/6250] loss: 0.212\n",
            "[2/10, 5700/6250] loss: 0.181\n",
            "[2/10, 5800/6250] loss: 0.201\n",
            "[2/10, 5900/6250] loss: 0.135\n",
            "[2/10, 6000/6250] loss: 0.197\n",
            "[2/10, 6100/6250] loss: 0.201\n",
            "[2/10, 6200/6250] loss: 0.193\n",
            "[2/10, 6250/6250] loss: 0.111\n",
            "[3/10, 100/6250] loss: 0.111\n",
            "[3/10, 200/6250] loss: 0.121\n",
            "[3/10, 300/6250] loss: 0.120\n",
            "[3/10, 400/6250] loss: 0.101\n",
            "[3/10, 500/6250] loss: 0.073\n",
            "[3/10, 600/6250] loss: 0.102\n",
            "[3/10, 700/6250] loss: 0.097\n",
            "[3/10, 800/6250] loss: 0.103\n",
            "[3/10, 900/6250] loss: 0.083\n",
            "[3/10, 1000/6250] loss: 0.085\n",
            "[3/10, 1100/6250] loss: 0.110\n",
            "[3/10, 1200/6250] loss: 0.113\n",
            "[3/10, 1300/6250] loss: 0.080\n",
            "[3/10, 1400/6250] loss: 0.101\n",
            "[3/10, 1500/6250] loss: 0.106\n",
            "[3/10, 1600/6250] loss: 0.102\n",
            "[3/10, 1700/6250] loss: 0.099\n",
            "[3/10, 1800/6250] loss: 0.098\n",
            "[3/10, 1900/6250] loss: 0.114\n",
            "[3/10, 2000/6250] loss: 0.120\n",
            "[3/10, 2100/6250] loss: 0.117\n",
            "[3/10, 2200/6250] loss: 0.132\n",
            "[3/10, 2300/6250] loss: 0.119\n",
            "[3/10, 2400/6250] loss: 0.132\n",
            "[3/10, 2500/6250] loss: 0.094\n",
            "[3/10, 2600/6250] loss: 0.144\n",
            "[3/10, 2700/6250] loss: 0.139\n",
            "[3/10, 2800/6250] loss: 0.126\n",
            "[3/10, 2900/6250] loss: 0.137\n",
            "[3/10, 3000/6250] loss: 0.115\n",
            "[3/10, 3100/6250] loss: 0.114\n",
            "[3/10, 3200/6250] loss: 0.142\n",
            "[3/10, 3300/6250] loss: 0.131\n",
            "[3/10, 3400/6250] loss: 0.090\n",
            "[3/10, 3500/6250] loss: 0.181\n",
            "[3/10, 3600/6250] loss: 0.129\n",
            "[3/10, 3700/6250] loss: 0.126\n",
            "[3/10, 3800/6250] loss: 0.155\n",
            "[3/10, 3900/6250] loss: 0.135\n",
            "[3/10, 4000/6250] loss: 0.148\n",
            "[3/10, 4100/6250] loss: 0.137\n",
            "[3/10, 4200/6250] loss: 0.087\n",
            "[3/10, 4300/6250] loss: 0.151\n",
            "[3/10, 4400/6250] loss: 0.130\n",
            "[3/10, 4500/6250] loss: 0.121\n",
            "[3/10, 4600/6250] loss: 0.132\n",
            "[3/10, 4700/6250] loss: 0.141\n",
            "[3/10, 4800/6250] loss: 0.146\n",
            "[3/10, 4900/6250] loss: 0.132\n",
            "[3/10, 5000/6250] loss: 0.129\n",
            "[3/10, 5100/6250] loss: 0.133\n",
            "[3/10, 5200/6250] loss: 0.173\n",
            "[3/10, 5300/6250] loss: 0.160\n",
            "[3/10, 5400/6250] loss: 0.163\n",
            "[3/10, 5500/6250] loss: 0.154\n",
            "[3/10, 5600/6250] loss: 0.144\n",
            "[3/10, 5700/6250] loss: 0.130\n",
            "[3/10, 5800/6250] loss: 0.153\n",
            "[3/10, 5900/6250] loss: 0.157\n",
            "[3/10, 6000/6250] loss: 0.186\n",
            "[3/10, 6100/6250] loss: 0.183\n",
            "[3/10, 6200/6250] loss: 0.178\n",
            "[3/10, 6250/6250] loss: 0.051\n",
            "[4/10, 100/6250] loss: 0.086\n",
            "[4/10, 200/6250] loss: 0.071\n",
            "[4/10, 300/6250] loss: 0.056\n",
            "[4/10, 400/6250] loss: 0.056\n",
            "[4/10, 500/6250] loss: 0.060\n",
            "[4/10, 600/6250] loss: 0.067\n",
            "[4/10, 700/6250] loss: 0.083\n",
            "[4/10, 800/6250] loss: 0.085\n",
            "[4/10, 900/6250] loss: 0.071\n",
            "[4/10, 1000/6250] loss: 0.065\n",
            "[4/10, 1100/6250] loss: 0.061\n",
            "[4/10, 1200/6250] loss: 0.062\n",
            "[4/10, 1300/6250] loss: 0.064\n",
            "[4/10, 1400/6250] loss: 0.083\n",
            "[4/10, 1500/6250] loss: 0.103\n",
            "[4/10, 1600/6250] loss: 0.113\n",
            "[4/10, 1700/6250] loss: 0.077\n",
            "[4/10, 1800/6250] loss: 0.062\n",
            "[4/10, 1900/6250] loss: 0.091\n",
            "[4/10, 2000/6250] loss: 0.087\n",
            "[4/10, 2100/6250] loss: 0.075\n",
            "[4/10, 2200/6250] loss: 0.087\n",
            "[4/10, 2300/6250] loss: 0.069\n",
            "[4/10, 2400/6250] loss: 0.091\n",
            "[4/10, 2500/6250] loss: 0.077\n",
            "[4/10, 2600/6250] loss: 0.061\n",
            "[4/10, 2700/6250] loss: 0.116\n",
            "[4/10, 2800/6250] loss: 0.076\n",
            "[4/10, 2900/6250] loss: 0.088\n",
            "[4/10, 3000/6250] loss: 0.082\n",
            "[4/10, 3100/6250] loss: 0.109\n",
            "[4/10, 3200/6250] loss: 0.133\n",
            "[4/10, 3300/6250] loss: 0.086\n",
            "[4/10, 3400/6250] loss: 0.094\n",
            "[4/10, 3500/6250] loss: 0.093\n",
            "[4/10, 3600/6250] loss: 0.081\n",
            "[4/10, 3700/6250] loss: 0.119\n",
            "[4/10, 3800/6250] loss: 0.114\n",
            "[4/10, 3900/6250] loss: 0.102\n",
            "[4/10, 4000/6250] loss: 0.106\n",
            "[4/10, 4100/6250] loss: 0.158\n",
            "[4/10, 4200/6250] loss: 0.091\n",
            "[4/10, 4300/6250] loss: 0.125\n",
            "[4/10, 4400/6250] loss: 0.123\n",
            "[4/10, 4500/6250] loss: 0.159\n",
            "[4/10, 4600/6250] loss: 0.121\n",
            "[4/10, 4700/6250] loss: 0.115\n",
            "[4/10, 4800/6250] loss: 0.107\n",
            "[4/10, 4900/6250] loss: 0.102\n",
            "[4/10, 5000/6250] loss: 0.069\n",
            "[4/10, 5100/6250] loss: 0.104\n",
            "[4/10, 5200/6250] loss: 0.100\n",
            "[4/10, 5300/6250] loss: 0.083\n",
            "[4/10, 5400/6250] loss: 0.130\n",
            "[4/10, 5500/6250] loss: 0.091\n",
            "[4/10, 5600/6250] loss: 0.121\n",
            "[4/10, 5700/6250] loss: 0.125\n",
            "[4/10, 5800/6250] loss: 0.172\n",
            "[4/10, 5900/6250] loss: 0.091\n",
            "[4/10, 6000/6250] loss: 0.170\n",
            "[4/10, 6100/6250] loss: 0.156\n",
            "[4/10, 6200/6250] loss: 0.130\n",
            "[4/10, 6250/6250] loss: 0.058\n",
            "[5/10, 100/6250] loss: 0.055\n",
            "[5/10, 200/6250] loss: 0.074\n",
            "[5/10, 300/6250] loss: 0.077\n",
            "[5/10, 400/6250] loss: 0.069\n",
            "[5/10, 500/6250] loss: 0.075\n",
            "[5/10, 600/6250] loss: 0.084\n",
            "[5/10, 700/6250] loss: 0.077\n",
            "[5/10, 800/6250] loss: 0.084\n",
            "[5/10, 900/6250] loss: 0.080\n",
            "[5/10, 1000/6250] loss: 0.066\n",
            "[5/10, 1100/6250] loss: 0.060\n",
            "[5/10, 1200/6250] loss: 0.071\n",
            "[5/10, 1300/6250] loss: 0.072\n",
            "[5/10, 1400/6250] loss: 0.047\n",
            "[5/10, 1500/6250] loss: 0.075\n",
            "[5/10, 1600/6250] loss: 0.087\n",
            "[5/10, 1700/6250] loss: 0.076\n",
            "[5/10, 1800/6250] loss: 0.086\n",
            "[5/10, 1900/6250] loss: 0.065\n",
            "[5/10, 2000/6250] loss: 0.060\n",
            "[5/10, 2100/6250] loss: 0.053\n",
            "[5/10, 2200/6250] loss: 0.094\n",
            "[5/10, 2300/6250] loss: 0.081\n",
            "[5/10, 2400/6250] loss: 0.074\n",
            "[5/10, 2500/6250] loss: 0.070\n",
            "[5/10, 2600/6250] loss: 0.090\n",
            "[5/10, 2700/6250] loss: 0.112\n",
            "[5/10, 2800/6250] loss: 0.068\n",
            "[5/10, 2900/6250] loss: 0.124\n",
            "[5/10, 3000/6250] loss: 0.102\n",
            "[5/10, 3100/6250] loss: 0.127\n",
            "[5/10, 3200/6250] loss: 0.079\n",
            "[5/10, 3300/6250] loss: 0.079\n",
            "[5/10, 3400/6250] loss: 0.103\n",
            "[5/10, 3500/6250] loss: 0.053\n",
            "[5/10, 3600/6250] loss: 0.072\n",
            "[5/10, 3700/6250] loss: 0.103\n",
            "[5/10, 3800/6250] loss: 0.117\n",
            "[5/10, 3900/6250] loss: 0.094\n",
            "[5/10, 4000/6250] loss: 0.098\n",
            "[5/10, 4100/6250] loss: 0.120\n",
            "[5/10, 4200/6250] loss: 0.118\n",
            "[5/10, 4300/6250] loss: 0.094\n",
            "[5/10, 4400/6250] loss: 0.102\n",
            "[5/10, 4500/6250] loss: 0.159\n",
            "[5/10, 4600/6250] loss: 0.137\n",
            "[5/10, 4700/6250] loss: 0.098\n",
            "[5/10, 4800/6250] loss: 0.101\n",
            "[5/10, 4900/6250] loss: 0.093\n",
            "[5/10, 5000/6250] loss: 0.103\n",
            "[5/10, 5100/6250] loss: 0.090\n",
            "[5/10, 5200/6250] loss: 0.103\n",
            "[5/10, 5300/6250] loss: 0.065\n",
            "[5/10, 5400/6250] loss: 0.070\n",
            "[5/10, 5500/6250] loss: 0.093\n",
            "[5/10, 5600/6250] loss: 0.091\n",
            "[5/10, 5700/6250] loss: 0.118\n",
            "[5/10, 5800/6250] loss: 0.179\n",
            "[5/10, 5900/6250] loss: 0.119\n",
            "[5/10, 6000/6250] loss: 0.143\n",
            "[5/10, 6100/6250] loss: 0.131\n",
            "[5/10, 6200/6250] loss: 0.193\n",
            "[5/10, 6250/6250] loss: 0.105\n",
            "[6/10, 100/6250] loss: 0.095\n",
            "[6/10, 200/6250] loss: 0.077\n",
            "[6/10, 300/6250] loss: 0.088\n",
            "[6/10, 400/6250] loss: 0.050\n",
            "[6/10, 500/6250] loss: 0.095\n",
            "[6/10, 600/6250] loss: 0.063\n",
            "[6/10, 700/6250] loss: 0.061\n",
            "[6/10, 800/6250] loss: 0.054\n",
            "[6/10, 900/6250] loss: 0.060\n",
            "[6/10, 1000/6250] loss: 0.054\n",
            "[6/10, 1100/6250] loss: 0.103\n",
            "[6/10, 1200/6250] loss: 0.053\n",
            "[6/10, 1300/6250] loss: 0.054\n",
            "[6/10, 1400/6250] loss: 0.095\n",
            "[6/10, 1500/6250] loss: 0.075\n",
            "[6/10, 1600/6250] loss: 0.071\n",
            "[6/10, 1700/6250] loss: 0.073\n",
            "[6/10, 1800/6250] loss: 0.060\n",
            "[6/10, 1900/6250] loss: 0.088\n",
            "[6/10, 2000/6250] loss: 0.076\n",
            "[6/10, 2100/6250] loss: 0.061\n",
            "[6/10, 2200/6250] loss: 0.090\n",
            "[6/10, 2300/6250] loss: 0.046\n",
            "[6/10, 2400/6250] loss: 0.057\n",
            "[6/10, 2500/6250] loss: 0.048\n",
            "[6/10, 2600/6250] loss: 0.057\n",
            "[6/10, 2700/6250] loss: 0.058\n",
            "[6/10, 2800/6250] loss: 0.111\n",
            "[6/10, 2900/6250] loss: 0.100\n",
            "[6/10, 3000/6250] loss: 0.086\n",
            "[6/10, 3100/6250] loss: 0.070\n",
            "[6/10, 3200/6250] loss: 0.106\n",
            "[6/10, 3300/6250] loss: 0.080\n",
            "[6/10, 3400/6250] loss: 0.052\n",
            "[6/10, 3500/6250] loss: 0.078\n",
            "[6/10, 3600/6250] loss: 0.066\n",
            "[6/10, 3700/6250] loss: 0.094\n",
            "[6/10, 3800/6250] loss: 0.051\n",
            "[6/10, 3900/6250] loss: 0.074\n",
            "[6/10, 4000/6250] loss: 0.070\n",
            "[6/10, 4100/6250] loss: 0.050\n",
            "[6/10, 4200/6250] loss: 0.059\n",
            "[6/10, 4300/6250] loss: 0.089\n",
            "[6/10, 4400/6250] loss: 0.091\n",
            "[6/10, 4500/6250] loss: 0.058\n",
            "[6/10, 4600/6250] loss: 0.096\n",
            "[6/10, 4700/6250] loss: 0.073\n",
            "[6/10, 4800/6250] loss: 0.071\n",
            "[6/10, 4900/6250] loss: 0.090\n",
            "[6/10, 5000/6250] loss: 0.082\n",
            "[6/10, 5100/6250] loss: 0.103\n",
            "[6/10, 5200/6250] loss: 0.119\n",
            "[6/10, 5300/6250] loss: 0.099\n",
            "[6/10, 5400/6250] loss: 0.058\n",
            "[6/10, 5500/6250] loss: 0.089\n",
            "[6/10, 5600/6250] loss: 0.078\n",
            "[6/10, 5700/6250] loss: 0.059\n",
            "[6/10, 5800/6250] loss: 0.075\n",
            "[6/10, 5900/6250] loss: 0.111\n",
            "[6/10, 6000/6250] loss: 0.131\n",
            "[6/10, 6100/6250] loss: 0.112\n",
            "[6/10, 6200/6250] loss: 0.090\n",
            "[6/10, 6250/6250] loss: 0.069\n",
            "[7/10, 100/6250] loss: 0.058\n",
            "[7/10, 200/6250] loss: 0.043\n",
            "[7/10, 300/6250] loss: 0.036\n",
            "[7/10, 400/6250] loss: 0.036\n",
            "[7/10, 500/6250] loss: 0.069\n",
            "[7/10, 600/6250] loss: 0.035\n",
            "[7/10, 700/6250] loss: 0.058\n",
            "[7/10, 800/6250] loss: 0.062\n",
            "[7/10, 900/6250] loss: 0.055\n",
            "[7/10, 1000/6250] loss: 0.069\n",
            "[7/10, 1100/6250] loss: 0.050\n",
            "[7/10, 1200/6250] loss: 0.040\n",
            "[7/10, 1300/6250] loss: 0.032\n",
            "[7/10, 1400/6250] loss: 0.041\n",
            "[7/10, 1500/6250] loss: 0.072\n",
            "[7/10, 1600/6250] loss: 0.074\n",
            "[7/10, 1700/6250] loss: 0.058\n",
            "[7/10, 1800/6250] loss: 0.069\n",
            "[7/10, 1900/6250] loss: 0.027\n",
            "[7/10, 2000/6250] loss: 0.048\n",
            "[7/10, 2100/6250] loss: 0.050\n",
            "[7/10, 2200/6250] loss: 0.056\n",
            "[7/10, 2300/6250] loss: 0.049\n",
            "[7/10, 2400/6250] loss: 0.061\n",
            "[7/10, 2500/6250] loss: 0.050\n",
            "[7/10, 2600/6250] loss: 0.064\n",
            "[7/10, 2700/6250] loss: 0.053\n",
            "[7/10, 2800/6250] loss: 0.059\n",
            "[7/10, 2900/6250] loss: 0.068\n",
            "[7/10, 3000/6250] loss: 0.071\n",
            "[7/10, 3100/6250] loss: 0.079\n",
            "[7/10, 3200/6250] loss: 0.055\n",
            "[7/10, 3300/6250] loss: 0.084\n",
            "[7/10, 3400/6250] loss: 0.074\n",
            "[7/10, 3500/6250] loss: 0.049\n",
            "[7/10, 3600/6250] loss: 0.039\n",
            "[7/10, 3700/6250] loss: 0.066\n",
            "[7/10, 3800/6250] loss: 0.102\n",
            "[7/10, 3900/6250] loss: 0.055\n",
            "[7/10, 4000/6250] loss: 0.047\n",
            "[7/10, 4100/6250] loss: 0.063\n",
            "[7/10, 4200/6250] loss: 0.074\n",
            "[7/10, 4300/6250] loss: 0.077\n",
            "[7/10, 4400/6250] loss: 0.093\n",
            "[7/10, 4500/6250] loss: 0.061\n",
            "[7/10, 4600/6250] loss: 0.066\n",
            "[7/10, 4700/6250] loss: 0.055\n",
            "[7/10, 4800/6250] loss: 0.057\n",
            "[7/10, 4900/6250] loss: 0.057\n",
            "[7/10, 5000/6250] loss: 0.038\n",
            "[7/10, 5100/6250] loss: 0.088\n",
            "[7/10, 5200/6250] loss: 0.135\n",
            "[7/10, 5300/6250] loss: 0.075\n",
            "[7/10, 5400/6250] loss: 0.122\n",
            "[7/10, 5500/6250] loss: 0.104\n",
            "[7/10, 5600/6250] loss: 0.077\n",
            "[7/10, 5700/6250] loss: 0.076\n",
            "[7/10, 5800/6250] loss: 0.088\n",
            "[7/10, 5900/6250] loss: 0.094\n",
            "[7/10, 6000/6250] loss: 0.135\n",
            "[7/10, 6100/6250] loss: 0.117\n",
            "[7/10, 6200/6250] loss: 0.090\n",
            "[7/10, 6250/6250] loss: 0.058\n",
            "[8/10, 100/6250] loss: 0.091\n",
            "[8/10, 200/6250] loss: 0.049\n",
            "[8/10, 300/6250] loss: 0.034\n",
            "[8/10, 400/6250] loss: 0.095\n",
            "[8/10, 500/6250] loss: 0.070\n",
            "[8/10, 600/6250] loss: 0.060\n",
            "[8/10, 700/6250] loss: 0.047\n",
            "[8/10, 800/6250] loss: 0.105\n",
            "[8/10, 900/6250] loss: 0.053\n",
            "[8/10, 1000/6250] loss: 0.107\n",
            "[8/10, 1100/6250] loss: 0.068\n",
            "[8/10, 1200/6250] loss: 0.054\n",
            "[8/10, 1300/6250] loss: 0.076\n",
            "[8/10, 1400/6250] loss: 0.064\n",
            "[8/10, 1500/6250] loss: 0.047\n",
            "[8/10, 1600/6250] loss: 0.056\n",
            "[8/10, 1700/6250] loss: 0.043\n",
            "[8/10, 1800/6250] loss: 0.053\n",
            "[8/10, 1900/6250] loss: 0.063\n",
            "[8/10, 2000/6250] loss: 0.067\n",
            "[8/10, 2100/6250] loss: 0.045\n",
            "[8/10, 2200/6250] loss: 0.058\n",
            "[8/10, 2300/6250] loss: 0.038\n",
            "[8/10, 2400/6250] loss: 0.047\n",
            "[8/10, 2500/6250] loss: 0.025\n",
            "[8/10, 2600/6250] loss: 0.045\n",
            "[8/10, 2700/6250] loss: 0.057\n",
            "[8/10, 2800/6250] loss: 0.038\n",
            "[8/10, 2900/6250] loss: 0.076\n",
            "[8/10, 3000/6250] loss: 0.044\n",
            "[8/10, 3100/6250] loss: 0.102\n",
            "[8/10, 3200/6250] loss: 0.070\n",
            "[8/10, 3300/6250] loss: 0.068\n",
            "[8/10, 3400/6250] loss: 0.041\n",
            "[8/10, 3500/6250] loss: 0.026\n",
            "[8/10, 3600/6250] loss: 0.059\n",
            "[8/10, 3700/6250] loss: 0.050\n",
            "[8/10, 3800/6250] loss: 0.078\n",
            "[8/10, 3900/6250] loss: 0.078\n",
            "[8/10, 4000/6250] loss: 0.099\n",
            "[8/10, 4100/6250] loss: 0.107\n",
            "[8/10, 4200/6250] loss: 0.135\n",
            "[8/10, 4300/6250] loss: 0.100\n",
            "[8/10, 4400/6250] loss: 0.085\n",
            "[8/10, 4500/6250] loss: 0.070\n",
            "[8/10, 4600/6250] loss: 0.086\n",
            "[8/10, 4700/6250] loss: 0.077\n",
            "[8/10, 4800/6250] loss: 0.049\n",
            "[8/10, 4900/6250] loss: 0.044\n",
            "[8/10, 5000/6250] loss: 0.040\n",
            "[8/10, 5100/6250] loss: 0.054\n",
            "[8/10, 5200/6250] loss: 0.060\n",
            "[8/10, 5300/6250] loss: 0.065\n",
            "[8/10, 5400/6250] loss: 0.062\n",
            "[8/10, 5500/6250] loss: 0.071\n",
            "[8/10, 5600/6250] loss: 0.080\n",
            "[8/10, 5700/6250] loss: 0.064\n",
            "[8/10, 5800/6250] loss: 0.086\n",
            "[8/10, 5900/6250] loss: 0.047\n",
            "[8/10, 6000/6250] loss: 0.061\n",
            "[8/10, 6100/6250] loss: 0.107\n",
            "[8/10, 6200/6250] loss: 0.111\n",
            "[8/10, 6250/6250] loss: 0.030\n",
            "[9/10, 100/6250] loss: 0.043\n",
            "[9/10, 200/6250] loss: 0.047\n",
            "[9/10, 300/6250] loss: 0.030\n",
            "[9/10, 400/6250] loss: 0.037\n",
            "[9/10, 500/6250] loss: 0.031\n",
            "[9/10, 600/6250] loss: 0.045\n",
            "[9/10, 700/6250] loss: 0.049\n",
            "[9/10, 800/6250] loss: 0.018\n",
            "[9/10, 900/6250] loss: 0.024\n",
            "[9/10, 1000/6250] loss: 0.031\n",
            "[9/10, 1100/6250] loss: 0.044\n",
            "[9/10, 1200/6250] loss: 0.026\n",
            "[9/10, 1300/6250] loss: 0.033\n",
            "[9/10, 1400/6250] loss: 0.054\n",
            "[9/10, 1500/6250] loss: 0.033\n",
            "[9/10, 1600/6250] loss: 0.037\n",
            "[9/10, 1700/6250] loss: 0.032\n",
            "[9/10, 1800/6250] loss: 0.054\n",
            "[9/10, 1900/6250] loss: 0.022\n",
            "[9/10, 2000/6250] loss: 0.038\n",
            "[9/10, 2100/6250] loss: 0.095\n",
            "[9/10, 2200/6250] loss: 0.057\n",
            "[9/10, 2300/6250] loss: 0.061\n",
            "[9/10, 2400/6250] loss: 0.054\n",
            "[9/10, 2500/6250] loss: 0.065\n",
            "[9/10, 2600/6250] loss: 0.072\n",
            "[9/10, 2700/6250] loss: 0.072\n",
            "[9/10, 2800/6250] loss: 0.097\n",
            "[9/10, 2900/6250] loss: 0.044\n",
            "[9/10, 3000/6250] loss: 0.038\n",
            "[9/10, 3100/6250] loss: 0.067\n",
            "[9/10, 3200/6250] loss: 0.068\n",
            "[9/10, 3300/6250] loss: 0.078\n",
            "[9/10, 3400/6250] loss: 0.068\n",
            "[9/10, 3500/6250] loss: 0.087\n",
            "[9/10, 3600/6250] loss: 0.053\n",
            "[9/10, 3700/6250] loss: 0.062\n",
            "[9/10, 3800/6250] loss: 0.076\n",
            "[9/10, 3900/6250] loss: 0.060\n",
            "[9/10, 4000/6250] loss: 0.056\n",
            "[9/10, 4100/6250] loss: 0.070\n",
            "[9/10, 4200/6250] loss: 0.054\n",
            "[9/10, 4300/6250] loss: 0.041\n",
            "[9/10, 4400/6250] loss: 0.051\n",
            "[9/10, 4500/6250] loss: 0.039\n",
            "[9/10, 4600/6250] loss: 0.072\n",
            "[9/10, 4700/6250] loss: 0.057\n",
            "[9/10, 4800/6250] loss: 0.055\n",
            "[9/10, 4900/6250] loss: 0.040\n",
            "[9/10, 5000/6250] loss: 0.032\n",
            "[9/10, 5100/6250] loss: 0.076\n",
            "[9/10, 5200/6250] loss: 0.045\n",
            "[9/10, 5300/6250] loss: 0.071\n",
            "[9/10, 5400/6250] loss: 0.048\n",
            "[9/10, 5500/6250] loss: 0.044\n",
            "[9/10, 5600/6250] loss: 0.064\n",
            "[9/10, 5700/6250] loss: 0.081\n",
            "[9/10, 5800/6250] loss: 0.044\n",
            "[9/10, 5900/6250] loss: 0.079\n",
            "[9/10, 6000/6250] loss: 0.101\n",
            "[9/10, 6100/6250] loss: 0.095\n",
            "[9/10, 6200/6250] loss: 0.069\n",
            "[9/10, 6250/6250] loss: 0.024\n",
            "[10/10, 100/6250] loss: 0.038\n",
            "[10/10, 200/6250] loss: 0.019\n",
            "[10/10, 300/6250] loss: 0.025\n",
            "[10/10, 400/6250] loss: 0.021\n",
            "[10/10, 500/6250] loss: 0.033\n",
            "[10/10, 600/6250] loss: 0.038\n",
            "[10/10, 700/6250] loss: 0.050\n",
            "[10/10, 800/6250] loss: 0.038\n",
            "[10/10, 900/6250] loss: 0.017\n",
            "[10/10, 1000/6250] loss: 0.029\n",
            "[10/10, 1100/6250] loss: 0.034\n",
            "[10/10, 1200/6250] loss: 0.057\n",
            "[10/10, 1300/6250] loss: 0.047\n",
            "[10/10, 1400/6250] loss: 0.031\n",
            "[10/10, 1500/6250] loss: 0.023\n",
            "[10/10, 1600/6250] loss: 0.049\n",
            "[10/10, 1700/6250] loss: 0.034\n",
            "[10/10, 1800/6250] loss: 0.084\n",
            "[10/10, 1900/6250] loss: 0.043\n",
            "[10/10, 2000/6250] loss: 0.042\n",
            "[10/10, 2100/6250] loss: 0.044\n",
            "[10/10, 2200/6250] loss: 0.026\n",
            "[10/10, 2300/6250] loss: 0.038\n",
            "[10/10, 2400/6250] loss: 0.027\n",
            "[10/10, 2500/6250] loss: 0.045\n",
            "[10/10, 2600/6250] loss: 0.025\n",
            "[10/10, 2700/6250] loss: 0.058\n",
            "[10/10, 2800/6250] loss: 0.055\n",
            "[10/10, 2900/6250] loss: 0.040\n",
            "[10/10, 3000/6250] loss: 0.038\n",
            "[10/10, 3100/6250] loss: 0.065\n",
            "[10/10, 3200/6250] loss: 0.033\n",
            "[10/10, 3300/6250] loss: 0.049\n",
            "[10/10, 3400/6250] loss: 0.042\n",
            "[10/10, 3500/6250] loss: 0.055\n",
            "[10/10, 3600/6250] loss: 0.031\n",
            "[10/10, 3700/6250] loss: 0.104\n",
            "[10/10, 3800/6250] loss: 0.039\n",
            "[10/10, 3900/6250] loss: 0.042\n",
            "[10/10, 4000/6250] loss: 0.088\n",
            "[10/10, 4100/6250] loss: 0.052\n",
            "[10/10, 4200/6250] loss: 0.079\n",
            "[10/10, 4300/6250] loss: 0.046\n",
            "[10/10, 4400/6250] loss: 0.045\n",
            "[10/10, 4500/6250] loss: 0.078\n",
            "[10/10, 4600/6250] loss: 0.040\n",
            "[10/10, 4700/6250] loss: 0.033\n",
            "[10/10, 4800/6250] loss: 0.045\n",
            "[10/10, 4900/6250] loss: 0.048\n",
            "[10/10, 5000/6250] loss: 0.037\n",
            "[10/10, 5100/6250] loss: 0.065\n",
            "[10/10, 5200/6250] loss: 0.068\n",
            "[10/10, 5300/6250] loss: 0.100\n",
            "[10/10, 5400/6250] loss: 0.110\n",
            "[10/10, 5500/6250] loss: 0.086\n",
            "[10/10, 5600/6250] loss: 0.054\n",
            "[10/10, 5700/6250] loss: 0.060\n",
            "[10/10, 5800/6250] loss: 0.054\n",
            "[10/10, 5900/6250] loss: 0.060\n",
            "[10/10, 6000/6250] loss: 0.068\n",
            "[10/10, 6100/6250] loss: 0.078\n",
            "[10/10, 6200/6250] loss: 0.064\n",
            "[10/10, 6250/6250] loss: 0.021\n",
            "Finished Training\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test\n",
        "# top 1 和 top 5的結果\n",
        "top1_correct = 0\n",
        "top5_correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testLoader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        labels_resize = labels.view(-1, 1)\n",
        "\n",
        "        _, top1_predicted = torch.max(outputs.data, 1)\n",
        "        _, top5_predicted = outputs.topk(5, 1, True, True)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        top1_correct += (top1_predicted == labels).sum().item()\n",
        "        top5_correct += (top5_predicted == labels_resize).sum().item()\n",
        "\n",
        "print('[Top 1] Accuracy of the network on the 10000 test inputs: %d %%' % (100 * top1_correct / total))\n",
        "print('[Top 5] Accuracy of the network on the 10000 test inputs: %d %%' % (100 * top5_correct / total))\n",
        "\n",
        "# 每個 class 的準確度\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in testLoader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(8):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "\n",
        "summary((net), (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXH-k7_jz7Gw",
        "outputId": "e0a383b8-56ff-462e-b3b1-b59f1aff9877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Top 1] Accuracy of the network on the 10000 test inputs: 72 %\n",
            "[Top 5] Accuracy of the network on the 10000 test inputs: 97 %\n",
            "Accuracy of plane : 72 %\n",
            "Accuracy of   car : 85 %\n",
            "Accuracy of  bird : 64 %\n",
            "Accuracy of   cat : 54 %\n",
            "Accuracy of  deer : 73 %\n",
            "Accuracy of   dog : 58 %\n",
            "Accuracy of  frog : 81 %\n",
            "Accuracy of horse : 77 %\n",
            "Accuracy of  ship : 83 %\n",
            "Accuracy of truck : 72 %\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           9,472\n",
            "         MaxPool2d-2           [-1, 64, 16, 16]               0\n",
            "            Conv2d-3          [-1, 128, 16, 16]         204,928\n",
            "         MaxPool2d-4            [-1, 128, 8, 8]               0\n",
            "            Linear-5                  [-1, 128]       1,048,704\n",
            "            Linear-6                   [-1, 64]           8,256\n",
            "            Linear-7                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 1,272,010\n",
            "Trainable params: 1,272,010\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.94\n",
            "Params size (MB): 4.85\n",
            "Estimated Total Size (MB): 5.80\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}